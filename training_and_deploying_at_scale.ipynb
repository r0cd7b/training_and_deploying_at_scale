{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1936bf3-dd02-4f0f-bd0a-797c5a7034f8",
   "metadata": {},
   "source": [
    "# 대규모 텐서플로 모델 훈련과 배포"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2781df-378f-449c-9dd6-ccc3c8c11832",
   "metadata": {},
   "source": [
    "먼저 몇 개의 모듈을 임포트한다. 맷플롯립 그림을 저장하는 함수를 준비한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df13166e-2610-4f28-8fee-6bcd0b9682a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 공통 모듈 임포트\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 깔끔한 그래프 출력을 위해\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# 그림을 저장할 위치\n",
    "PROJECT_ROOT_DIR = '.'\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, 'images')\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension='png', resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + '.' + fig_extension)\n",
    "    print('그림 저장' + fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cd7cdd-41d4-4591-b8c5-c46fe2ccff8b",
   "metadata": {},
   "source": [
    "## 텐서플로 모델 서빙"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5ece4-73eb-499a-be19-82c16a9d05e8",
   "metadata": {},
   "source": [
    "### 텐서플로 서빙 사용하기\n",
    "REST API나 gRPC API를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942f335-9ee5-4f3e-83af-9c423bc57328",
   "metadata": {},
   "source": [
    "**SavedModel로 내보내기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01190f46-cd6f-4339-ac60-4755fe068fa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 08:15:20.280497: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train_full = X_train_full[..., np.newaxis].astype(np.float32) / 255.\n",
    "X_test = X_test[..., np.newaxis].astype(np.float32) / 255.\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_new = X_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e06ca493-6c6d-406a-b088-897d565350fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 08:15:24.215555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:24.450481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:24.450725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:24.453581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:24.453765: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:24.453937: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:26.142923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:26.143291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:26.143308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-21 08:15:26.143572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:26.143635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5385 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:06:00.0, compute capability: 8.6\n",
      "2023-08-21 08:15:29.243208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-21 08:15:29.288653: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f8714041240 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-21 08:15:29.288690: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Ti, Compute Capability 8.6\n",
      "2023-08-21 08:15:32.507171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8903\n",
      "2023-08-21 08:15:32.777295: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-08-21 08:15:32.967414: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 12s 3ms/step - loss: 0.7010 - accuracy: 0.8198 - val_loss: 0.3745 - val_accuracy: 0.8970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f880c134b20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [keras.layers.Flatten(input_shape=[28, 28, 1]), keras.layers.Dense(100, 'relu'), keras.layers.Dense(10, 'softmax')]\n",
    ")\n",
    "model.compile(keras.optimizers.experimental.SGD(), 'sparse_categorical_crossentropy', ['accuracy'])\n",
    "# model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1cc6be3-a814-4532-bb6a-8e53c16484d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99, 0.  , 0.01],\n",
       "       [0.05, 0.  , 0.74, 0.03, 0.  , 0.08, 0.08, 0.  , 0.02, 0.  ],\n",
       "       [0.  , 0.94, 0.02, 0.01, 0.  , 0.  , 0.01, 0.01, 0.01, 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(model.predict(X_new), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d8f293c-b70c-4dd7-9bd3-16d22e76ee2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_mnist_model/0001'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_version = '0001'\n",
    "model_name = 'my_mnist_model'\n",
    "model_path = os.path.join(model_name, model_version)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71da1f24-1e21-46ee-aa24-838b750c7682",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm -rf {model_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc9bfdb-d5a5-4d4e-97de-1af67da8d710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_mnist_model/0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_mnist_model/0001/assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.saved_model.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "823a66c6-282c-4386-b0a6-7babc339d300",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_mnist_model/\n",
      "    0001/\n",
      "        fingerprint.pb\n",
      "        saved_model.pb\n",
      "        assets/\n",
      "        variables/\n",
      "            variables.data-00000-of-00001\n",
      "            variables.index\n"
     ]
    }
   ],
   "source": [
    "for root, _, files in os.walk(model_name):\n",
    "    indent = '    ' * root.count(os.sep)\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    for filename in files:\n",
    "        print(f'{indent}    {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9eeaa47-f869-4976-a8d4-1755a2b65efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-21 08:15:39.794518: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-21 08:15:41.048459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:41.073743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:41.074078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "The given SavedModel contains the following tag-sets:\n",
      "'serve'\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {model_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ffe2698-12f5-48e1-a072-93d1ef899f94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-21 08:15:42.674465: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-21 08:15:43.816225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:43.832320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:43.832573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
      "SignatureDef key: \"__saved_model_init_op\"\n",
      "SignatureDef key: \"serving_default\"\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {model_path} --tag_set serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "993b397d-74c6-4347-b0d9-6ac9f5b9fa66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-21 08:15:44.604485: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-21 08:15:45.993620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:46.015223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:46.015645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['flatten_input'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 28, 28, 1)\n",
      "      name: serving_default_flatten_input:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['dense_1'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 10)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {model_path} --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa510157-e6ff-4d97-91e6-a2f43bd12301",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-21 08:15:47.383297: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-21 08:15:49.586343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:49.603327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:49.603598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['flatten_input'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 28, 28, 1)\n",
      "        name: serving_default_flatten_input:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['dense_1'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 10)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "The MetaGraph with tag set ['serve'] contains the following ops: {'Placeholder', 'AssignVariableOp', 'DisableCopyOnRead', 'Relu', 'SaveV2', 'Softmax', 'VarHandleOp', 'MatMul', 'ReadVariableOp', 'MergeV2Checkpoints', 'StringJoin', 'StaticRegexFullMatch', 'BiasAdd', 'StatefulPartitionedCall', 'Const', 'NoOp', 'Identity', 'Reshape', 'Select', 'RestoreV2', 'ShardedFilename', 'Pack'}\n",
      "2023-08-21 08:15:49.640663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:49.640896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:49.641073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:50.139591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:50.139862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:50.139892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-21 08:15:50.140124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:50.140178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4888 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:06:00.0, compute capability: 8.6\n",
      "\n",
      "Concrete Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_input')\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: True\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          flatten_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='flatten_input')\n",
      "        Argument #2\n",
      "          DType: bool\n",
      "          Value: False\n",
      "        Argument #3\n",
      "          DType: NoneType\n",
      "          Value: None\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {model_path} --all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf99ca52-75dd-46b2-92c3-bc6de182ec60",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "X_new를 `npy` 파일로 만들면 모델에 쉽게 전달할 수 있다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d3dab56-a202-4289-9c67-458416dbe1ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.save('my_mnist_tests.npy', X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1db07fac-b303-402d-8865-d9c294566d2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flatten_input'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_name = model.input_names[0]\n",
    "input_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b867c28-a3e8-4014-a6c9-64293f06fa94",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "그리고 이제 `saved_model_cli`를 사용해 방금 저장한 샘플에 대한 예측을 만든다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7a34bcf-15af-43a9-838b-2c5bd3409cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-21 08:15:51.104219: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-21 08:15:52.291379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:52.304585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:52.304806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:52.328610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:52.328892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:52.329108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:52.832586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:52.832803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:52.832830: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-21 08:15:52.833047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-21 08:15:52.833105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4932 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:06:00.0, compute capability: 8.6\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/tools/saved_model_cli.py:689: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.saved_model.load` instead.\n",
      "W0821 08:15:52.833981 140644626757440 deprecation.py:364] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/tools/saved_model_cli.py:689: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.saved_model.load` instead.\n",
      "INFO:tensorflow:Restoring parameters from my_mnist_model/0001/variables/variables\n",
      "I0821 08:15:52.863677 140644626757440 saver.py:1413] Restoring parameters from my_mnist_model/0001/variables/variables\n",
      "2023-08-21 08:15:52.866893: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-08-21 08:15:59.308416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Result for output key dense_1:\n",
      "[[4.0297979e-04 5.8820883e-06 3.6691118e-04 1.3538052e-03 2.4600959e-04\n",
      "  1.2352725e-04 6.2443423e-06 9.8686582e-01 1.9353299e-04 1.0435285e-02]\n",
      " [4.7288459e-02 2.0101340e-03 7.4188489e-01 2.6382485e-02 4.3505319e-05\n",
      "  8.3596461e-02 7.7808000e-02 1.3808849e-05 2.0808099e-02 1.6411752e-04]\n",
      " [9.3498337e-04 9.3536335e-01 2.2831900e-02 1.0006595e-02 2.0743958e-03\n",
      "  3.7915485e-03 5.9907110e-03 5.2404716e-03 1.0138198e-02 3.6278213e-03]]\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli run --dir {model_path} --tag_set serve --signature_def serving_default --inputs {input_name}=my_mnist_tests.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9698d547-428e-42eb-94b4-e28ca5cb10b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(\n",
    "    [\n",
    "        [\n",
    "            1.1540909e-04,\n",
    "            6.7565077e-07,\n",
    "            6.4367504e-04,\n",
    "            1.3884673e-03,\n",
    "            7.0954684e-07,\n",
    "            1.6306719e-04,\n",
    "            2.9787778e-08,\n",
    "            9.9733573e-01,\n",
    "            4.6408379e-05,\n",
    "            3.0576065e-04\n",
    "        ],\n",
    "        [\n",
    "            2.2494975e-03,\n",
    "            7.9745638e-05,\n",
    "            9.8004192e-01,\n",
    "            1.2274164e-02,\n",
    "            2.3486223e-08,\n",
    "            9.9441968e-04,\n",
    "            2.5695174e-03,\n",
    "            1.5748299e-09,\n",
    "            1.7906638e-03,\n",
    "            4.0853223e-08\n",
    "        ],\n",
    "        [\n",
    "            7.1588256e-06,\n",
    "            9.8235393e-01,\n",
    "            5.3347293e-03,\n",
    "            2.8666973e-03,\n",
    "            2.3666114e-04,\n",
    "            6.4616540e-04,\n",
    "            1.0684166e-03,\n",
    "            5.2083088e-03,\n",
    "            2.1845093e-03,\n",
    "            9.3470750e-05\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ecdee9-cc77-4d8f-bca7-7acbab4ab399",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**텐서플로 서빙 설치하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12624aa-43f5-40cb-87d5-8ccadc05cb62",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "[도커](https://docs.docker.com/install/)가 없다면 설치한다. 그리고 다음을 실행한다:\n",
    "\n",
    "```bash\n",
    "docker pull tensorflow/serving:latest-gpu\n",
    "\n",
    "export ML_PATH=$HOME/ml # 또는 이 프로젝트가 있는 곳\n",
    "docker run -it --rm -p 8500:8500 -p 8501:8501 -v \"$ML_PATH/my_mnist_model:/models/my_mnist_model\" -e MODEL_NAME=my_mnist_model tensorflow/serving:latest-gpu\n",
    "```\n",
    "\n",
    "사용이 끝나면 Ctrl-C를 눌러 서버를 종료한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b438e87-1546-456e-8cd9-2716be8c6d4c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "또는 `tensorflow_model_server`가 설치되어 있다면 (예를 들어, 이 노트북을 코랩에서 실행하는 경우) 다음 세 개의 셀을 실행하여 서버를 시작한다:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ed973-c347-45c2-b427-420c1aefd264",
   "metadata": {},
   "source": [
    "```python\n",
    "os.environ['MODEL_DIR'] = os.path.split(os.path.abspath(model_path))[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51cdce5-d64e-4334-8889-79799521aa14",
   "metadata": {},
   "source": [
    "```python\n",
    "%%bash --bg\n",
    "nohup tensorflow_model_server --rest_api_port=8501 --model_name=my_mnist_model --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e57b70-4255-4a25-ab3b-c99e2cac5344",
   "metadata": {},
   "source": [
    "```python\n",
    "!tail server.log\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9c20f9-3ef7-4d84-96d9-6bfebf50405e",
   "metadata": {},
   "source": [
    "**REST API로 TF 서빙에 쿼리하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6f1796a-c923-48e9-9321-ad10f2a57dcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_data_json = json.dumps({\"signature_name\": \"serving_default\", \"instances\": X_new.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb340a9a-76f5-46eb-9687-7fcecc22f0fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'{\"signature_name\": \"serving_default\", \"instances\": [[[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.32941177487373...'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(input_data_json)[:1500] + '...'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193e141e-90f2-4c5b-82d9-f1139257ce14",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "이제 텐서플로 서빙의 REST API를 사용해 예측을 만든다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a11a0ee4-ff6b-45b9-9567-5dc439c7a811",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "SERVER_URL = 'http://172.17.0.3:8501/v1/models/my_mnist_model:predict'\n",
    "response = requests.post(SERVER_URL, input_data_json)\n",
    "response.raise_for_status()  # 에러가 생길 경우 예외를 발생한다.\n",
    "response = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09d78c76-d1de-46c2-acc1-51a7fa9cc652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['predictions'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9327a37a-af89-4877-8cdf-6a6d63a00860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99, 0.  , 0.01],\n",
       "       [0.05, 0.  , 0.74, 0.03, 0.  , 0.08, 0.08, 0.  , 0.02, 0.  ],\n",
       "       [0.  , 0.94, 0.02, 0.01, 0.  , 0.  , 0.01, 0.01, 0.01, 0.  ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = np.array(response['predictions'])\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9161b6f4-43a8-4e74-8d2e-db0be2b9c839",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**gRPC API로 TF 서빙에 쿼리하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad38e828-88af-49cc-ada7-54f566e576e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
    "\n",
    "request = PredictRequest()\n",
    "request.model_spec.name = model_name\n",
    "request.model_spec.signature_name = 'serving_default'\n",
    "input_name = model.input_names[0]\n",
    "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6cec6512-d76a-4aa5-b6bd-688569ccbb82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import grpc\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel('172.17.0.3:8500')\n",
    "predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "response = predict_service.Predict(request, timeout=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60ffbc37-b9af-4454-9d65-1318ee97f2e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_spec {\n",
       "  name: \"my_mnist_model\"\n",
       "  version {\n",
       "    value: 1\n",
       "  }\n",
       "  signature_name: \"serving_default\"\n",
       "}\n",
       "outputs {\n",
       "  key: \"dense_1\"\n",
       "  value {\n",
       "    dtype: DT_FLOAT\n",
       "    tensor_shape {\n",
       "      dim {\n",
       "        size: 3\n",
       "      }\n",
       "      dim {\n",
       "        size: 10\n",
       "      }\n",
       "    }\n",
       "    float_val: 0.000402979786\n",
       "    float_val: 5.88208832e-06\n",
       "    float_val: 0.000366911176\n",
       "    float_val: 0.00135380519\n",
       "    float_val: 0.000246009586\n",
       "    float_val: 0.00012352725\n",
       "    float_val: 6.24434233e-06\n",
       "    float_val: 0.986865819\n",
       "    float_val: 0.000193533\n",
       "    float_val: 0.010435285\n",
       "    float_val: 0.0472884588\n",
       "    float_val: 0.00201013405\n",
       "    float_val: 0.741884887\n",
       "    float_val: 0.0263824854\n",
       "    float_val: 4.35053189e-05\n",
       "    float_val: 0.0835964605\n",
       "    float_val: 0.077808\n",
       "    float_val: 1.3808849e-05\n",
       "    float_val: 0.0208080988\n",
       "    float_val: 0.000164117519\n",
       "    float_val: 0.000934983371\n",
       "    float_val: 0.935363352\n",
       "    float_val: 0.0228319\n",
       "    float_val: 0.0100065954\n",
       "    float_val: 0.00207439577\n",
       "    float_val: 0.00379154854\n",
       "    float_val: 0.00599071104\n",
       "    float_val: 0.00524047157\n",
       "    float_val: 0.0101381978\n",
       "    float_val: 0.00362782134\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238dbfea-338f-4d22-b8b4-4d13ab14806d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "응답을 텐서로 변환한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6828776-9594-4e39-b6b7-770b43e2ef71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99, 0.  , 0.01],\n",
       "       [0.05, 0.  , 0.74, 0.03, 0.  , 0.08, 0.08, 0.  , 0.02, 0.  ],\n",
       "       [0.  , 0.94, 0.02, 0.01, 0.  , 0.  , 0.01, 0.01, 0.01, 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_name = model.output_names[0]\n",
    "outputs_proto = response.outputs[output_name]\n",
    "y_proba = tf.make_ndarray(outputs_proto)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e371f65-9add-41ea-9c62-0c8d2c35404b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "클라이언트가 텐서플로 라이브러리를 사용하지 않는다면 넘파이 배열로 변환한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65f17131-684d-44bf-b8e9-9ce7817c8655",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99, 0.  , 0.01],\n",
       "       [0.05, 0.  , 0.74, 0.03, 0.  , 0.08, 0.08, 0.  , 0.02, 0.  ],\n",
       "       [0.  , 0.94, 0.02, 0.01, 0.  , 0.  , 0.01, 0.01, 0.01, 0.  ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_name = model.output_names[0]\n",
    "outputs_proto = response.outputs[output_name]\n",
    "shape = [dim.size for dim in outputs_proto.tensor_shape.dim]\n",
    "y_proba = np.array(outputs_proto.float_val).reshape(shape)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b04d18-68d6-40f7-8c0c-6207651babdf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**새로운 버전의 모델 배포하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3706a0fb-e877-451b-9d4c-d9a97c91df6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.7323 - accuracy: 0.7955 - val_loss: 0.3438 - val_accuracy: 0.9082\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3241 - accuracy: 0.9070 - val_loss: 0.2710 - val_accuracy: 0.9276\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2712 - accuracy: 0.9218 - val_loss: 0.2377 - val_accuracy: 0.9352\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2388 - accuracy: 0.9307 - val_loss: 0.2107 - val_accuracy: 0.9418\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2138 - accuracy: 0.9381 - val_loss: 0.1985 - val_accuracy: 0.9454\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.1950 - accuracy: 0.9436 - val_loss: 0.1762 - val_accuracy: 0.9544\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1777 - accuracy: 0.9480 - val_loss: 0.1661 - val_accuracy: 0.9560\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.1643 - accuracy: 0.9518 - val_loss: 0.1605 - val_accuracy: 0.9566\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.1517 - accuracy: 0.9561 - val_loss: 0.1442 - val_accuracy: 0.9608\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.1412 - accuracy: 0.9586 - val_loss: 0.1358 - val_accuracy: 0.9640\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
    "        keras.layers.Dense(50, 'relu'),\n",
    "        keras.layers.Dense(50, 'relu'),\n",
    "        keras.layers.Dense(10, 'softmax')\n",
    "    ]\n",
    ")\n",
    "model.compile(keras.optimizers.experimental.SGD(), 'sparse_categorical_crossentropy', ['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea3454d9-1696-4c94-97b5-935f0ca2952f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_mnist_model/0002'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_version = '0002'\n",
    "model_name = 'my_mnist_model'\n",
    "model_path = os.path.join(model_name, model_version)\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a400cd14-3ccc-4688-a527-60f750ebe1e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_mnist_model/0002/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_mnist_model/0002/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cca4efc-65f9-47a3-a08f-64d7e772aead",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_mnist_model/\n",
      "    0001/\n",
      "        fingerprint.pb\n",
      "        saved_model.pb\n",
      "        assets/\n",
      "        variables/\n",
      "            variables.data-00000-of-00001\n",
      "            variables.index\n",
      "    0002/\n",
      "        fingerprint.pb\n",
      "        saved_model.pb\n",
      "        assets/\n",
      "        variables/\n",
      "            variables.data-00000-of-00001\n",
      "            variables.index\n"
     ]
    }
   ],
   "source": [
    "for root, _, files in os.walk(model_name):\n",
    "    indent = '    ' * root.count(os.sep)\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    for filename in files:\n",
    "        print(f'{indent}    {filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ab993-1505-4f0f-86fd-21261c39b4ed",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**경고**: 새로운 모델이 텐서플로 서빙에 로드되기 전까지 잠시 기다려야 할 수 있다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "195ce5b3-35b0-4cb6-85c5-2094e837dde0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SERVER_URL = 'http://172.17.0.3:8501/v1/models/my_mnist_model:predict'\n",
    "response = requests.post(SERVER_URL, input_data_json)\n",
    "response.raise_for_status()\n",
    "response = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e2472cc-c628-49ac-be95-61fa7db01f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['predictions'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2410fa2-2977-474d-b89e-30e0e777c00f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99, 0.  , 0.01],\n",
       "       [0.05, 0.  , 0.74, 0.03, 0.  , 0.08, 0.08, 0.  , 0.02, 0.  ],\n",
       "       [0.  , 0.94, 0.02, 0.01, 0.  , 0.  , 0.01, 0.01, 0.01, 0.  ]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = np.array(response['predictions'])\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c69f4-2e82-4751-93f0-88cd82cbdde5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 예측 서비스 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b273d5-c49a-4a51-a0cb-beb8dd71027a",
   "metadata": {},
   "source": [
    "**구글 클라우드 클라이언트 라이브러리**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48813479-fc1b-49eb-8b55-15415298890b",
   "metadata": {},
   "source": [
    "구글 클라우드 AI 플랫폼에 모델을 배포하고, 서비스 계정의 개인키를 다운로드하여 프로젝트 디렉토리에 저장한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bbf270a-323e-455e-b8e4-1081c824b567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"mineral-trainer-394903-4289c79f86b1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "947a749c-a9d0-4ef0-a430-bae2a68e4808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "\n",
    "def predict_custom_trained_model_sample(\n",
    "        project, endpoint_id, instances, location='us-central1', api_endpoint='us-central1-aiplatform.googleapis.com'\n",
    "):\n",
    "    client_options = {'api_endpoint': api_endpoint}\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    instances = instances if isinstance(instances, list) else [instances]\n",
    "    instances = [json_format.ParseDict(instance_dict, Value()) for instance_dict in instances]\n",
    "    parameters_dict = {}\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    endpoint = client.endpoint_path(project=project, location=location, endpoint=endpoint_id)\n",
    "    response = client.predict(endpoint=endpoint, instances=instances, parameters=parameters)\n",
    "    predictions = response.predictions\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2426b16e-471a-4f19-bb35-c85856f8eb79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_probas = predict_custom_trained_model_sample(\n",
    "    '759555954445',\n",
    "    '8228789002740695040',\n",
    "    X_new.tolist(),\n",
    "    'asia-northeast3',\n",
    "    'asia-northeast3-aiplatform.googleapis.com'\n",
    ")\n",
    "np.round(Y_probas, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7919125c-b854-4345-baaf-191ea36c5a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
